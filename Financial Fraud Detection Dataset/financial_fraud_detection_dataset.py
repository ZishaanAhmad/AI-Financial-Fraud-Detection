# -*- coding: utf-8 -*-
"""Financial Fraud Detection Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mtb8Q-roRNVGu8ZRGvkdKsiAb1GPLFg1
"""

from google.colab import drive
drive.mount('/content/drive')

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

file_path = '/content/drive/MyDrive/Financial_Datasets/Synthetic_Financial_datasets_log.csv'
# Read the CSV file from the current directory
df = pd.read_csv(file_path, low_memory=False)

# Display the first few rows
df.head()

df.shape

df.describe()

"""# DELETING 100% NULL NUMERIC COLUMNS"""

nan_col = []
for i in df.columns:
  if df[i].dtype == 'float64' and df[i].describe()['count'] == 0.0:
    nan_col.append(i)
# nan_col # NAN numeric columns

#Drop 81 NAN numeric columns
df.drop(nan_col, axis=1, inplace=True)

df.head(2)

df.shape

"""# Deleting columns with only 1 value
because there will not be contributing in decision making
"""

def stats_generate(df):
    stats = []
    columns_names = ['Column Name', 'Unique_values', 'Percentage of NULL values', 'data type']
    for col in df.columns:
      unique = df[col].nunique()
      null_Percentage = df[col].isnull().sum() * 100 / df.shape[0]
      dataType = df[col].dtype
      stats.append((col,unique ,null_Percentage,dataType ))

    stats_df = pd.DataFrame(stats, columns=columns_names)
    # stats_df.sort_values('Percentage of NULL values', ascending=False)
    return stats_df
Generated_Stats = stats_generate(df)

Generated_Stats.head(50)

col= "Percentage of NULL values"
analysis = (Generated_Stats[col] <= 10).sum()
a = ((Generated_Stats[col]!=0) & (Generated_Stats[col]<=10)).sum()
b = ((Generated_Stats[col]>10) & (Generated_Stats[col]<=50)).sum()
c = (Generated_Stats[col]>50).sum()
print("There are:\n{} columns without missing values\n{} columns with less than 10% of missing values\n {} columns with missing values between 10% and 50%\n {} columns with more than 50% of missing values".format(analysis,a,b,c))

labels =["No missing data", "Missing 0-10%", "Missing 10-50%", "Missing over 50% of data"]
fig1, ax1 = plt.subplots(figsize=(8,8))
ax1.pie([analysis,a,b,c], autopct='%1.1f%%',labels=labels, textprops={'fontsize': 15})
ax1.axis('equal')
plt.show()

unique1_col = []
for i in df.columns:
  if df[i].nunique() == 1:
    unique1_col.append(i)
unique1_col # Having 1 unique columns

#Drop unique 1 value columns
df.drop(unique1_col, axis=1, inplace=True)
df.shape

df.head()

"""# Deleting NULL values columns other than numeric"""

null90_col = []
for i in df.columns:
  if df[i].isnull().sum() * 100 / df.shape[0] >= 90.0:
    null90_col.append(i)
null90_col # Having 90% null columns

#Drop 90% null value columns
df.drop(null90_col, axis=1, inplace=True)
df.shape

df.head()

def Catergories(df,col):
  print("---",col,"---")
  print("Column unique values: ",df[col].unique())
  print("nan values: ",sum(pd.isnull(df[col])))
  df[col].value_counts().plot(kind = 'bar');
  plt.title(col)
  plt.ylabel('Number of Occurrences', fontsize=12)
  plt.xlabel(col, fontsize=12)
  plt.show()

# df.dropna(subset=['loan_status'], inplace=True)
df.shape

Catergories(df,'type')

# Catergories(df,'amount')

Catergories(df,'step')

Catergories(df,'isFraud')

# Catergories(df,'oldbalanceOrg')

# Catergories(df,'newbalanceOrg')

# Catergories(df,'oldbalanceDest')

# Catergories(df,'newbalanceOrg')

# # Catergories(df,'sub_grade')
# col = 'type'
# print("Column unique values: ",df[col].unique())
# print("nan values: ",sum(pd.isnull(df[col])))
# count  = df[col].value_counts()
# plt.figure(figsize=(15,8))
# sns.barplot(count.index, count.values, alpha=0.8)
# plt.title(col)
# plt.ylabel('Number of Occurrences', fontsize=12)
# plt.xlabel(col, fontsize=12)
# plt.show()

col = 'type'
df[col].value_counts().plot.pie(labels= df[col].unique() , labeldistance = 0.7,figsize=(10, 10))
plt.title(col)
plt.show()

df.head()

# df['zip_code'] = df['zip_code'].str.rstrip('x')
# df.int_rate = df.int_rate.str.rstrip('%')
# df.revol_util = df.revol_util.str.rstrip('%')
# df.head(2)

def convertToCat(df,col):
  df[col] = df[col].astype('category')
  df[col]= df[col].cat.codes

categorical_Col= ['type']
for i in categorical_Col:
  convertToCat(df,i)

df.head()

# convertToCat(df, 'loan_status')

df.shape

def correlation(col1,col2,col3):
  sns.heatmap(df[[col1,col2,col3]].corr(method ='pearson'), cmap='RdBu_r', annot=True, center=0.5, vmin =0, vmax=1)
  plt.title('Correlation between' +' '+col1+ '~'+' '+ col2 +'columns\n')
  plt.show()

def correlation2(col1,col2):
  sns.heatmap(df[[col1,col2]].corr(method ='pearson'), cmap='RdBu_r', annot=True, center=0.5, vmin =0, vmax=1)
  plt.title('Correlation between' +' '+col1+ '~'+' '+ col2 +'columns\n')
  plt.show()

def correlation3(col1,col2,col3, col4):
  sns.heatmap(df[[col1,col2,col3, col4]].corr(method ='pearson'), cmap='RdBu_r', annot=True, center=0.5, vmin =0, vmax=1)
  plt.title('Correlation between' +' '+col1+ '~'+' '+ col2 +'columns\n')
  plt.show()

# correlation2('nameOrig','nameDest')

correlation('oldbalanceOrg','newbalanceOrig', 'oldbalanceDest')

correlation('oldbalanceOrg','newbalanceOrig', 'newbalanceDest')

correlation('oldbalanceOrg','newbalanceDest', 'oldbalanceDest')

correlation('newbalanceOrig','newbalanceDest', 'oldbalanceDest')

df.drop(columns=['nameOrig', 'nameDest'])
df.head()

def get_skewwed_dataframe(df):
    pd.options.display.float_format = '{:,.4f}'.format
    sk_df = pd.DataFrame([{'column': c,
                          'uniq': df[c].nunique(),
                          'skewness': df[c].value_counts(normalize=True).values[0] * 100
                          } for c in df.columns])
    sk_df = sk_df.sort_values('skewness', ascending=False)
    return sk_df

_skewwed_data = get_skewwed_dataframe(df)

_skewwed_data.head(11)

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
    roc_auc_score,
    make_scorer,
    f1_score
)
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier

from IPython.display import display

# Fill nulls if any
df_model = df.fillna(0)

# üéØ Features & Target
target = 'isFraud'
features = [col for col in df_model.columns if col != target]

X = df_model[features]
y = df_model[target]

# üß™ Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42)

# üìä Class Ratio Check
print("üîç Class Distribution:")
print(y.value_counts(normalize=True))

# üß† Custom F1 scorer
f1_scorer = make_scorer(f1_score)

# # üîß Model Dictionary with Hyperparameters (first & last values)
# models = {
#     "XGBoost": {
#         "model": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
#         "params": {
#             "n_estimators": [100, 500],
#             "learning_rate": [0.01, 0.3],
#             "max_depth": [3, 10],
#             "subsample": [0.5, 1.0],
#             "colsample_bytree": [0.5, 1.0],
#             "scale_pos_weight": [1, 100]  # for imbalance
#         }
#     },
#     "LightGBM": {
#         "model": LGBMClassifier(random_state=42),
#         "params": {
#             "n_estimators": [100, 500],
#             "learning_rate": [0.01, 0.3],
#             "max_depth": [3, 10],
#             "subsample": [0.5, 1.0],
#             "colsample_bytree": [0.5, 1.0],
#             "scale_pos_weight": [1, 100]
#         }
#     },
#     "RandomForest": {
#         "model": RandomForestClassifier(random_state=42),
#         "params": {
#             "n_estimators": [100, 500],
#             "max_depth": [5, 20],
#             "max_features": ["sqrt", "log2"]
#         }
#     },
#     "GradientBoosting": {
#         "model": GradientBoostingClassifier(random_state=42),
#         "params": {
#             "n_estimators": [100, 300],
#             "learning_rate": [0.01, 0.2],
#             "max_depth": [3, 10],
#             "subsample": [0.5, 1.0]
#         }
#     },
#     "AdaBoost": {
#         "model": AdaBoostClassifier(random_state=42),
#         "params": {
#             "n_estimators": [50, 300],
#             "learning_rate": [0.01, 1.0]
#         }
#     },
#     "LogisticRegression": {
#         "model": LogisticRegression(max_iter=1000, solver='lbfgs'),
#         "params": {
#             "C": [0.01, 10],
#             "class_weight": [None, 'balanced']
#         }
#     }
# }

# # üß™ Run GridSearchCV
# results = []

# for name, mp in models.items():
#     print(f"üîç Tuning: {name}")

#     grid = GridSearchCV(
#         mp["model"],
#         mp["params"],
#         scoring=f1_scorer,
#         cv=3,
#         n_jobs=-1,
#         verbose=0,
#         return_train_score=False
#     )

#     grid.fit(X_train, y_train)
#     best_model = grid.best_estimator_

#     # üß† Predict
#     y_pred = best_model.predict(X_test)
#     y_proba = best_model.predict_proba(X_test)[:, 1] if hasattr(best_model, "predict_proba") else None

#     # üìä Metrics
#     f1 = f1_score(y_test, y_pred)
#     roc = roc_auc_score(y_test, y_proba) if y_proba is not None else None

#     # üîÅ Cross-Validation
#     cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring=f1_scorer, n_jobs=-1)

#     # üìã Append results
#     results.append({
#         "Model": name,
#         "F1 Score": round(f1, 4),
#         "ROC-AUC": round(roc, 4) if roc else "N/A",
#         "CV F1 Mean": round(np.mean(cv_scores), 4),
#         "CV F1 Std": round(np.std(cv_scores), 4),
#         "Best Params": grid.best_params_
#     })

#     print(f"‚úÖ Done: {name}")
#     print("F1:", round(f1, 4), "| ROC-AUC:", round(roc, 4) if roc else "N/A")

"""# Chunk 1"""

from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.metrics import f1_score, roc_auc_score
import pandas as pd
import numpy as np

# Define models for this chunk
models_chunk_1 = {
    "XGBoost": {
        "model": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
        "params": {
            "n_estimators": [100, 300],
            "learning_rate": [0.01, 0.1],
            "max_depth": [3, 10],
            "subsample": [0.5, 1.0],
            "colsample_bytree": [0.5, 1.0],
            "scale_pos_weight": [1, 100]
        }
    },
    "LightGBM": {
        "model": LGBMClassifier(random_state=42),
        "params": {
            "n_estimators": [100, 300],
            "learning_rate": [0.01, 0.1],
            "max_depth": [3, 10],
            "subsample": [0.5, 1.0],
            "colsample_bytree": [0.5, 1.0],
            "scale_pos_weight": [1, 100]
        }
    }
}

results = []

for name, mp in models_chunk_1.items():
    print(f"üîç Tuning: {name}")

    # üîß Define GridSearchCV
    print("üîß Setting up GridSearchCV...")
    grid = GridSearchCV(
        mp["model"],
        mp["params"],
        scoring=f1_scorer,
        cv=3,                    # 3-fold CV is sufficient in most cases
        n_jobs=2,                # Reduced to 2 jobs to prevent memory crash
        verbose=0,
        return_train_score=False
    )

    # üöÄ Fit GridSearchCV
    print("üöÄ Running grid search...")
    grid.fit(X_train, y_train)
    best_model = grid.best_estimator_
    print("‚úÖ Grid search complete.")
    print(f"üìå Best Params: {grid.best_params_}")

    # ü§ñ Make predictions
    print("ü§ñ Making predictions on test set...")
    y_pred = best_model.predict(X_test)
    y_proba = best_model.predict_proba(X_test)[:, 1] if hasattr(best_model, "predict_proba") else None

    # üìä Evaluation
    print("üìä Evaluating performance...")
    f1 = f1_score(y_test, y_pred)
    roc = roc_auc_score(y_test, y_proba) if y_proba is not None else None
    # cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring=f1_scorer, n_jobs=-1)

    # üì• Append results
    print("üìù Saving results...")
    results.append({
        "Model": name,
        "F1 Score": round(f1, 4),
        "ROC-AUC": round(roc, 4) if roc else "N/A",
        # "CV F1 Mean": round(np.mean(cv_scores), 4),
        # "CV F1 Std": round(np.std(cv_scores), 4),
        "Best Params": grid.best_params_,
        "Trained Model": best_model
    })

    print(f"‚úÖ Done: {name}")
    print("F1:", round(f1, 4), "| ROC-AUC:", round(roc, 4) if roc else "N/A")

# # Show sorted table
# results_df = pd.DataFrame(results)
# results_df = results_df.sort_values(by="F1 Score", ascending=False)
# results_df[["Model", "F1 Score", "ROC-AUC", "CV F1 Mean", "CV F1 Std", "Best Params"]]

results_df = pd.DataFrame(results)

# Sort by F1
results_df_sorted = results_df.sort_values(by='F1 Score', ascending=False).reset_index(drop=True)

# Show top models
results_df_sorted[['Model', 'F1 Score', 'ROC-AUC', 'CV F1 Mean', 'CV F1 Std']]

"""# Chunk 2"""

models_chunk_2 = {
    "RandomForest": {
        "model": RandomForestClassifier(random_state=42),
        "params": {
            "n_estimators": [100, 300],
            "max_depth": [5, 20],
            "max_features": ["sqrt", "log2"]
        }
    },
    "GradientBoosting": {
        "model": GradientBoostingClassifier(random_state=42),
        "params": {
            "n_estimators": [100, 300],
            "learning_rate": [0.01, 0.2],
            "max_depth": [3, 10],
            "subsample": [0.5, 1.0]
        }
    }
}

results = []

for name, mp in models_chunk_2.items():
    print(f"üîç Tuning: {name}")

    # üîß Define GridSearchCV
    print("üîß Setting up GridSearchCV...")
    grid = GridSearchCV(
        mp["model"],
        mp["params"],
        scoring=f1_scorer,
        cv=3,                    # 3-fold CV is sufficient in most cases
        n_jobs=2,                # Reduced to 2 jobs to prevent memory crash
        verbose=0,
        return_train_score=False
    )

    # üöÄ Fit GridSearchCV
    print("üöÄ Running grid search...")
    grid.fit(X_train, y_train)
    best_model = grid.best_estimator_
    print("‚úÖ Grid search complete.")
    print(f"üìå Best Params: {grid.best_params_}")

    # ü§ñ Make predictions
    print("ü§ñ Making predictions on test set...")
    y_pred = best_model.predict(X_test)
    y_proba = best_model.predict_proba(X_test)[:, 1] if hasattr(best_model, "predict_proba") else None

    # üìä Evaluation
    print("üìä Evaluating performance...")
    f1 = f1_score(y_test, y_pred)
    roc = roc_auc_score(y_test, y_proba) if y_proba is not None else None
    # cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring=f1_scorer, n_jobs=-1)

    # üì• Append results
    print("üìù Saving results...")
    results.append({
        "Model": name,
        "F1 Score": round(f1, 4),
        "ROC-AUC": round(roc, 4) if roc else "N/A",
        # "CV F1 Mean": round(np.mean(cv_scores), 4),
        # "CV F1 Std": round(np.std(cv_scores), 4),
        "Best Params": grid.best_params_,
        "Trained Model": best_model
    })

    print(f"‚úÖ Done: {name}")
    print("F1:", round(f1, 4), "| ROC-AUC:", round(roc, 4) if roc else "N/A")

# results_df = pd.DataFrame(results)
# results_df = results_df.sort_values(by="F1 Score", ascending=False)
# results_df[["Model", "F1 Score", "ROC-AUC", "CV F1 Mean", "CV F1 Std", "Best Params"]]

results_df = pd.DataFrame(results)

# Sort by F1
results_df_sorted = results_df.sort_values(by='F1 Score', ascending=False).reset_index(drop=True)

# Show top models
results_df_sorted[['Model', 'F1 Score', 'ROC-AUC', 'CV F1 Mean', 'CV F1 Std']]

"""# Chunk 3"""

models_chunk_3 = {
    "AdaBoost": {
        "model": AdaBoostClassifier(random_state=42),
        "params": {
            "n_estimators": [50, 300],
            "learning_rate": [0.01, 0.1]
        }
    },
    "LogisticRegression": {
        "model": LogisticRegression(max_iter=1000, solver='lbfgs'),
        "params": {
            "C": [0.01, 10],
            "class_weight": [None, 'balanced']
        }
    }
}

results = []

for name, mp in models_chunk_3.items():
    print(f"üîç Tuning: {name}")

    # üîß Define GridSearchCV
    print("üîß Setting up GridSearchCV...")
    grid = GridSearchCV(
        mp["model"],
        mp["params"],
        scoring=f1_scorer,
        cv=3,                    # 3-fold CV is sufficient in most cases
        n_jobs=2,                # Reduced to 2 jobs to prevent memory crash
        verbose=0,
        return_train_score=False
    )

    # üöÄ Fit GridSearchCV
    print("üöÄ Running grid search...")
    grid.fit(X_train, y_train)
    best_model = grid.best_estimator_
    print("‚úÖ Grid search complete.")
    print(f"üìå Best Params: {grid.best_params_}")

    # ü§ñ Make predictions
    print("ü§ñ Making predictions on test set...")
    y_pred = best_model.predict(X_test)
    y_proba = best_model.predict_proba(X_test)[:, 1] if hasattr(best_model, "predict_proba") else None

    # üìä Evaluation
    print("üìä Evaluating performance...")
    f1 = f1_score(y_test, y_pred)
    roc = roc_auc_score(y_test, y_proba) if y_proba is not None else None
    # cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring=f1_scorer, n_jobs=-1)

    # üì• Append results
    print("üìù Saving results...")
    results.append({
        "Model": name,
        "F1 Score": round(f1, 4),
        "ROC-AUC": round(roc, 4) if roc else "N/A",
        # "CV F1 Mean": round(np.mean(cv_scores), 4),
        # "CV F1 Std": round(np.std(cv_scores), 4),
        "Best Params": grid.best_params_,
        "Trained Model": best_model
    })

    print(f"‚úÖ Done: {name}")
    print("F1:", round(f1, 4), "| ROC-AUC:", round(roc, 4) if roc else "N/A")

# results_df = pd.DataFrame(results)
# results_df = results_df.sort_values(by="F1 Score", ascending=False)
# results_df[["Model", "F1 Score", "ROC-AUC", "CV F1 Mean", "CV F1 Std", "Best Params"]]

results_df = pd.DataFrame(results)

# Sort by F1
results_df_sorted = results_df.sort_values(by='F1 Score', ascending=False).reset_index(drop=True)

# Show top models
results_df_sorted[['Model', 'F1 Score', 'ROC-AUC', 'CV F1 Mean', 'CV F1 Std']]

# Uncomment if you want to see the best parameters too
display(results_df_sorted[["Model", "Best Params"]])

# üîé Confusion Matrix for Best Model
# best_result = max(results, key=lambda x: x['F1 Score'])
# best_model_name = best_result['Model']
# print(f"\nüîç Best Model: {best_model_name}")


from sklearn.metrics import confusion_matrix

# üîç Get the best result (top F1 Score)
best_result = results_df_sorted.iloc[0]
best_model_name = best_result['Model']
final_model = best_result['Trained Model']  # üëà Retrieve trained model directly

print(f"\n‚úÖ Best Model: {best_model_name}")
print("üîß Best Params:", best_result['Best Params'])

# üß† Predict
y_pred = final_model.predict(X_test)

# üìâ Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
cm_df = pd.DataFrame(cm,
                     index=["Actual Not Fraud", "Actual Fraud"],
                     columns=["Predicted Not Fraud", "Predicted Fraud"])

print("üî¢ Confusion Matrix:")
display(cm_df.style.background_gradient(cmap='Purples'))

# üî≥ Confusion Matrix Plot
ConfusionMatrixDisplay(cm, display_labels=["Not Fraud", "Fraud"]).plot(cmap='Purples')
plt.title("üîç Confusion Matrix")
plt.show()

# üî¨ Feature Importances (if supported)
if hasattr(final_model, "feature_importances_"):
    importances = final_model.feature_importances_
    indices = np.argsort(importances)[::-1]

    feat_df = pd.DataFrame({
        'Feature': np.array(features)[indices],
        'Importance': importances[indices]
    })

    print("\nüí° Top 10 Feature Importances:")
    display(feat_df.head(10).style.background_gradient(cmap='Greens'))

    # üìä Plot
    plt.figure(figsize=(12, 6))
    sns.barplot(x='Importance', y='Feature', data=feat_df.head(10), palette="Greens_d")
    plt.title(f"üîç Top Feature Importances for {best_model_name}")
    plt.xlabel("Importance")
    plt.ylabel("Feature")
    plt.tight_layout()
    plt.show()
else:
    print(f"‚ö†Ô∏è Model {best_model_name} does not support feature importances.")

# plt.figure(figsize=(12,6))
# plt.title("üîç Feature Importances")
# sns.barplot(x='Importance', y='Feature', data=feat_df.head(10))
# plt.show()

# # ‚öñÔ∏è Precision-Recall vs Threshold Plot
# precision, recall, thresholds = precision_recall_curve(y_test, y_proba)

# plt.figure(figsize=(10,6))
# plt.plot(thresholds, precision[:-1], label='Precision')
# plt.plot(thresholds, recall[:-1], label='Recall')
# plt.xlabel("Threshold")
# plt.ylabel("Score")
# plt.title("Precision vs Recall vs Threshold")
# plt.legend()
# plt.grid()
# plt.show()

# üîÅ Cross-Validation (F1 Score)
f1_scorer = make_scorer(f1_score)

cv_scores = cross_val_score(
    model, X_train, y_train,
    cv=5, scoring=f1_scorer, n_jobs=-1
)

cv_df = pd.DataFrame({
    'Fold': [f'Fold {i+1}' for i in range(len(cv_scores))],
    'F1 Score': cv_scores
})

print("\nüîÅ Cross-Validation F1 Scores:")
display(cv_df.style.background_gradient(cmap='Oranges'))
print("üìä Mean F1 Score:", round(np.mean(cv_scores), 4))